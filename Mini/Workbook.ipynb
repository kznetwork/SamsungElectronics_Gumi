{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63a37df-24c9-418e-bfa4-dd7d917c06d7",
   "metadata": {},
   "source": [
    "# ÏõåÌÅ¨-Î∂Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b1daf-0900-4a1d-8e6e-44578e946cbd",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d1d9b2-ec63-4c0d-ad40-96e212637461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 15:02:12.707027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552bfce-83d2-43ba-aa46-9de9b3169f81",
   "metadata": {},
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f3a0ad-0d8c-4a7e-8e87-f021d6a5939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic' # Windows\n",
    "# matplotlib.rcParams['font.family'] = 'AppleGothic' # Mac\n",
    "matplotlib.rcParams['font.size'] = 15 # Í∏ÄÏûê ÌÅ¨Í∏∞\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÇ¨Ïö© Ïãú, ÎßàÏù¥ÎÑàÏä§ Í∏ÄÏûêÍ∞Ä Íπ®ÏßÄÎäî ÌòÑÏÉÅÏùÑ Ìï¥Í≤∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796f5fdc-de25-449f-ae54-83a73271b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.5 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc250fdb-84d7-4c76-8607-81ccf9231391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "3.5.0\n",
      "1.26.4\n",
      "2.1.4\n",
      "3.10.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "\n",
    "# this prints the library version\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "\n",
    "# this prints the python version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d20dbc-56c5-4b1e-bbe8-5eca06665359",
   "metadata": {},
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47edc5d5-941b-42d4-b795-f2dbd442c1b8",
   "metadata": {},
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8586393-6e48-4a44-8f41-96a48fdf37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌôòÍ≤ΩÎ≥ÄÏàò Ï§ÄÎπÑ\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<OpenAI_APIÏùò API ÌÇ§>\"\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7886278-8cb7-429b-ac44-59683aeccbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d129c61d-307f-4ffd-bfce-2ab35cf718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6c4fd-66bd-4c95-8261-204d9d9b4012",
   "metadata": {},
   "source": [
    "## „Äé1„Äè Îû≠Ï≤¥Ïù∏ Ïã§Ïäµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262d266-b019-4d64-bc79-bd52c1ddce2b",
   "metadata": {},
   "source": [
    "### Í∞ÑÎã®Ìïú ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ï≤¥Ïù∏ ÎßåÎì§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21a88d3-1bca-4b16-8cd2-b0fd79ff8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25639181-b20d-4c20-8952-e4d414900895",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "chain = prompt_template | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a200cd-eb90-4fcd-8b41-e3690a338141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'topic': 'cat'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdda4c-d024-479f-9768-ab3d4b49d4fd",
   "metadata": {},
   "source": [
    "### ÌÖçÏä§Ìä∏ Í∞êÏÑ± Î∂ÑÎ•ò Ï≤¥Ïù∏ ÎßåÎì§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64a9352-2969-4307-bd51-11410a0c0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de55cbea-c159-4836-8f40-7358b75ede81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(Enum):\n",
    "    POSITIVE = 'positive'\n",
    "    NEGATIVE = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b15b6496-9841-4821-9e5d-dd4c0db6413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\"Classify '{input_text}' as 'positive' or 'negative' only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2e5875-728d-4c97-a6d1-820b54cc2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | ChatOpenAI() | EnumOutputParser(enum=Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b31ff7-eae6-40d1-b6d7-060113bdb3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "Sentiment.NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'input_text':'I love LLM.'})\n",
    "print(result) \n",
    "\n",
    "result = chain.invoke({'input_text':'I hate LLM.'})\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "505c1719-6059-4c85-bc1c-83e8a93b8809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "Sentiment.NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'input_text':'I love and hate LLM.'})\n",
    "print(result) \n",
    "\n",
    "result = chain.invoke({'input_text':'I hate and love LLM.'})\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c496eb-ec75-40f5-919d-922b5f3422ad",
   "metadata": {},
   "source": [
    "### Í∞ÑÎã®Ìïú Ï±óÎ¥á ÎßåÎì§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64aee1a5-7091-42f0-9655-d0d68ba8af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e98fd3c-63e9-4687-adb2-9112a6211d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Talk like a close friend and use emojis üòä\"),\n",
    "    (\"human\", \"{user_input}\")]\n",
    ")\n",
    "\n",
    "chain = {\"user_input\": RunnablePassthrough()} | prompt_template | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4480dc1a-89d2-4fe8-8550-764a708c1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ÏïàÎÖïÌïòÏÑ∏Ïöî?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > ÏïàÎÖï~ Ïñ¥ÎñªÍ≤å ÏßÄÎÇ¥? üòä Ïò§ÎûúÎßåÏù¥Ïïº~ Î≠ê Ìï¥?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "        \n",
    "    print(\" A I > \", end=\"\", flush=True)\n",
    "    for chunk in chain.stream(user_message):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa84e5-aabf-490c-8678-243b18c7d5ce",
   "metadata": {},
   "source": [
    "### ÏàòÌïô Ïó∞ÏÇ∞ Ï≤¥Ïù∏ Íµ¨ÌòÑÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc990b-d044-4e60-a548-44eb87060a95",
   "metadata": {},
   "source": [
    "pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e237fb0-2f0c-4645-af43-2faa58a30ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c372a35-3022-4bed-81f2-960a0e05282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"For {query}, write only the mathematical expression suitable for numexpr.evaluate().\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4da895-d05e-4121-b50c-62ded4a2a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | ChatOpenAI() | StrOutputParser() | numexpr.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b19fe60-395d-465a-947f-b5c9b3175e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'query': '4ÏôÄ 5Î•º ÎçîÌï¥Ï§ò.'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915df925-deba-40cb-952a-847ada9043c3",
   "metadata": {},
   "source": [
    "### PythonREPLÏùÑ ÌôúÏö©Ìïú ÎèôÏ†Å ÏΩîÎìú Ïã§Ìñâ Ï±óÎ¥á Íµ¨ÌòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11da63fd-d08f-401b-b73c-f22d561c303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcadfd5c-352c-4f63-814f-a72dc84c509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "ÏÇ¨Ïö©ÏûêÏùò Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÌååÏù¥Ïç¨ ÏΩîÎìúÎ•º ÏûëÏÑ±Ìï©ÎãàÎã§.\n",
    "ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÌôïÏù∏Ïù¥ÎÇò Ï∂îÍ∞Ä Ï†ïÎ≥¥Î•º Îã§Ïãú Î¨ªÏßÄ ÎßàÏÑ∏Ïöî.\n",
    "ÏßÅÏ†ë Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÌååÏù¥Ïç¨ ÏΩîÎìúÎßå Î∞òÌôòÌïòÏÑ∏Ïöî.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template),\n",
    "     (\"human\", \"{user_input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9f94ae8-b052-4ed9-a394-7130d4478539",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"user_input\": RunnablePassthrough()} | prompt_template | ChatOpenAI() | StrOutputParser() | PythonREPL().run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9e7279a-795f-4384-a5f0-2c3cf948f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_user(user_message):\n",
    "    max_attempts = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            ai_message = chain.invoke(user_message)\n",
    "            if \"error\" in ai_message.lower() or \"failed\" in ai_message.lower():\n",
    "                raise Exception(f\"Detected error in AI message: {ai_message}\")\n",
    "            return ai_message\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            print(f\"Attempt {attempts}: Error - {e}\")\n",
    "            if attempts == max_attempts:\n",
    "                return \"Sorry, I am unable to process your request at the moment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4835d532-a6db-430f-96f6-7ce25179557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ÏïàÎÖï!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: Error - Detected error in AI message: SyntaxError('invalid syntax', ('<string>', 1, 6, 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?\\n', 1, 7))\n",
      "Attempt 2: Error - Detected error in AI message: SyntaxError('invalid syntax', ('<string>', 1, 6, 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?\\n', 1, 7))\n",
      "Attempt 3: Error - Detected error in AI message: SyntaxError('invalid syntax', ('<string>', 1, 6, 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?\\n', 1, 7))\n",
      " A I > Sorry, I am unable to process your request at the moment.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ÌïúÍ∏ÄÎÇ†ÏùÑ ÏïåÎ†§ Ï§ò\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > ÌïúÍ∏ÄÎÇ†ÏùÄ 10Ïõî 9ÏùºÏûÖÎãàÎã§.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd0164-aef0-4485-9eaf-e54620022919",
   "metadata": {},
   "source": [
    "### RAG Ï≤¥Ïù∏ Íµ¨ÏÑ±-PDF Î¨∏ÏÑú Í∏∞Î∞ò QAÏ±óÎ¥á Íµ¨ÌòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac9d7f0-68b1-402f-9a70-fbe0973e2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# PDF ÌååÏùºÏù¥ Ï†ÄÏû•Îêú Ìè¥Îçî Í≤ΩÎ°úÎ•º ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "folder_path = './datasets/pdfs/'  \n",
    "\n",
    "# ÌÖçÏä§Ìä∏Î•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏Î•º Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.\n",
    "texts = []\n",
    "\n",
    "# ÌÖçÏä§Ìä∏Î•º Î∂ÑÌï†Ìï† Îïå ÏÇ¨Ïö©Ìï† CharacterTextSplitter Í∞ùÏ≤¥Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\", # ÌÖçÏä§Ìä∏Î•º Î∂ÑÌï†Ìï† Îïå ÏÇ¨Ïö©Ìï† Íµ¨Î∂ÑÏûê\n",
    "    chunk_size = 1000, # Í∞Å Î∂ÑÌï†Îêú ÌÖçÏä§Ìä∏Ïùò ÏµúÎåÄ Í∏∏Ïù¥\n",
    "    chunk_overlap = 50) # Î∂ÑÌï†Îêú ÌÖçÏä§Ìä∏ Í∞ÑÏùò Ï§ëÏ≤© Í∏∏Ïù¥\n",
    "\n",
    "# ÏßÄÏ†ïÎêú Ìè¥Îçî ÎÇ¥Ïùò Î™®Îì† ÌååÏùºÏùÑ ÏàúÌöåÌï©ÎãàÎã§.\n",
    "for filename in os.listdir(folder_path):\n",
    "    # ÌååÏùºÏù¥ PDF ÌòïÏãùÏù∏ Í≤ΩÏö∞ÏóêÎßå Ï≤òÎ¶¨Ìï©ÎãàÎã§.\n",
    "    if filename.endswith(\".pdf\"): \n",
    "        # PDF ÌååÏùºÏùÑ Î°úÎìúÌïòÏó¨ raw_documentsÏóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "        raw_documents = PyPDFLoader(folder_path + '/' + filename).load()\n",
    "        # Î°úÎìúÎêú Î¨∏ÏÑúÎ•º Î∂ÑÌï†ÌïòÏó¨ documentsÏóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "        # Î∂ÑÌï†Îêú Î¨∏ÏÑúÎ•º texts Î¶¨Ïä§Ìä∏Ïóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "        texts.extend(documents)\n",
    "\n",
    "# Î∂ÑÌï†Îêú ÌÖçÏä§Ìä∏Î•º EmbeddingsÎ°ú Î≥ÄÌôòÌïòÏó¨ Chroma Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings())\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Í≤ÄÏÉâÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÎäî retriever Í∞ùÏ≤¥Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "# search_kwargs: Í≤ÄÏÉâ Ïãú ÏÇ¨Ïö©Ìï† Ï∂îÍ∞Ä Îß§Í∞úÎ≥ÄÏàò (Ïó¨Í∏∞ÏÑúÎäî ÏÉÅÏúÑ 10Í∞úÏùò Í≤∞Í≥ºÎ•º Î∞òÌôò)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08059c7a-03ab-41da-8872-77e46662efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"ÎãπÏã†ÏùÄ ÏßàÎ¨∏ ÎãµÎ≥Ä ÏûëÏóÖÏùò Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. Îã§Ïùå Î¨∏Îß•ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî. ÎãµÏùÑ Î™®Î•∏Îã§Î©¥ Î™®Î•∏Îã§Í≥† ÎßêÌïòÏÑ∏Ïöî. ÎãµÎ≥ÄÏùÄ ÏµúÎåÄ ÏÑ∏ Î¨∏Ïû•ÏúºÎ°ú ÏûëÏÑ±ÌïòÍ≥† Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥Î•º Ìè¨Ìï®ÌïòÏó¨ Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±ÌïòÏÑ∏Ïöî. ÌïúÍµ≠Ïñ¥Î°ú ÏûëÏÑ±Ìï©ÎãàÎã§. \\ns\\nÏßàÎ¨∏: {question} \\nÎ¨∏Îß•: {context} \\nÎãµÎ≥Ä:\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ade58fa7-43cd-42da-916e-aee7d60befbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  LLM Î™®Îç∏Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥ Ï§ò.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > LLMÏùÄ Large Language ModelsÏùò ÏïΩÏûêÎ°ú, ÎåÄÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÇ¨Ï†Ñ ÌõàÎ†®Îêú Í±∞ÎåÄÌïú Ïñ∏Ïñ¥ Î™®Îç∏ÏùÑ Í∞ÄÎ¶¨ÌÇµÎãàÎã§. LLMÏùÄ Îã§ÏñëÌïú ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ ÏûëÏóÖÏóê ÎåÄÌïú ÏßÄÏãùÏùÑ Ìè¨Í¥ÑÏ†ÅÏúºÎ°ú Î≥¥Ïú†ÌïòÍ≥† ÏûàÏúºÎ©∞, ÏßÄÏãù ÏßëÏïΩÏ†ÅÏù∏ ÏûëÏóÖÏù¥ÎÇò ÏûêÏó∞Ïñ¥ Ïù¥Ìï¥, ÏÉùÏÑ± ÏûëÏóÖ Îì±ÏóêÏÑú Îõ∞Ïñ¥ÎÇú ÏÑ±Í≥ºÎ•º Î≥¥Ïùº Ïàò ÏûàÏäµÎãàÎã§. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥: ÌéòÏù¥ÏßÄ 1, ÏÜåÏä§: './datasets/pdfs//Harnessing the Power of LLMs in Practice A Survey on ChatGPT and Beyond-2304.13712v2.pdf'.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "def chat_with_user(user_message):\n",
    "    ai_message = chain.invoke(user_message)\n",
    "    return ai_message\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018778b7-5aed-4c62-b5b6-c070af733d0e",
   "metadata": {},
   "source": [
    "### Memory-ÎåÄÌôîÌòï Ï±óÎ¥á ÎßåÎì§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bd576d-71c7-42a4-a125-49980f420589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feff4bdc-fe55-40d4-a294-dddc7759ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5867/2204816263.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplateÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎåÄÌôîÏùò Í∏∞Î≥∏ ÌÖúÌîåÎ¶øÏùÑ ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "# ÏãúÏä§ÌÖú Î©îÏãúÏßÄÏôÄ ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Talk like a close friend and use emojis üòä\"),  # ÏãúÏä§ÌÖú Î©îÏãúÏßÄ: ÏπúÍ∑ºÌïú ÏπúÍµ¨Ï≤òÎüº ÎßêÌïòÍ≥† Ïù¥Î™®ÏßÄÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # ÎåÄÌôî Í∏∞Î°ùÏùÑ ÏúÑÌïú ÏûêÎ¶¨ ÌëúÏãúÏûê\n",
    "    (\"human\", \"{user_input}\")  # ÏÇ¨Ïö©ÏûêÏùò ÏûÖÎ†•ÏùÑ Ìè¨Ìï®\n",
    "])\n",
    "\n",
    "# ÎåÄÌôî Î©îÎ™®Î¶¨Î•º ÏÑ§Ï†ïÌï©ÎãàÎã§. ÏµúÍ∑º 3Í∞úÏùò Î©îÏãúÏßÄÎ•º Î∞òÌôòÌïòÎèÑÎ°ù ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "\n",
    "# ÎåÄÌôî Ï≤¥Ïù∏ÏùÑ ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "chain = (\n",
    "    { \"user_input\": RunnablePassthrough() }  # ÏÇ¨Ïö©ÏûêÏùò ÏûÖÎ†•ÏùÑ Í∑∏ÎåÄÎ°ú Ï†ÑÎã¨\n",
    "    | RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )  # Î©îÎ™®Î¶¨ÏóêÏÑú ÎåÄÌôî Í∏∞Î°ùÏùÑ Î∂àÎü¨ÏôÄ chat_history Î≥ÄÏàòÏóê Ìï†Îãπ\n",
    "    | prompt_template  # ÏÑ§Ï†ïÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©\n",
    "    | ChatOpenAI()  # OpenAIÏùò Ï±óÎ¥á Î™®Îç∏ÏùÑ ÏÇ¨Ïö©\n",
    "    | StrOutputParser()  # Î¨∏ÏûêÏó¥ Ï∂úÎ†• ÌååÏÑú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59076c58-68be-41a5-a310-56811a2610ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ÏïàÎÖï?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > ÏïàÎÖï~ üòä Ïñ¥ÎñªÍ≤å ÏßÄÎÇ¥? Î≠ê ÌïòÍ≥† ÏûàÏñ¥?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "def chat_with_user(user_message):\n",
    "    ai_message = chain.invoke(user_message)\n",
    "    memory.save_context({\"input\": user_message}, {\"output\": ai_message})\n",
    "    return ai_message\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be695ceb-80a9-40c1-b3da-0e32a8beb097",
   "metadata": {},
   "source": [
    "### ÎèÑÍµ¨ Ï≤¥Ïù∏ Íµ¨Ï∂ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb495ab5-8bc0-4daa-b73d-d42365f05268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c548a7ae-21b7-48b5-b5b6-e1240ddfc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_weather(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"\n",
    "    Open-Meteo APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏßÄÏ†ïÎêú ÏúÑÎèÑÏôÄ Í≤ΩÎèÑÏùò ÌòÑÏû¨ ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current_weather\": True,\n",
    "        \"daily\": \"temperature_2m_max,temperature_2m_min\",\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    info = {}\n",
    "    if response.status_code == 200:\n",
    "        info = response.json()\n",
    "    return json.dumps(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca60dd7f-8e94-429b-b136-91f3c5d48b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_location() -> str:\n",
    "    \"\"\"\n",
    "    IPinfo.io APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ IP Ï£ºÏÜå Í∏∞Î∞òÏùò ÌòÑÏû¨ ÏúÑÏπòÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
    "    \"\"\"\n",
    "    response = requests.get('https://ipinfo.io')\n",
    "    info = response.json()\n",
    "    return json.dumps(info)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "tools = [get_current_weather, get_current_location]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22554554-9e8e-4a53-805e-719fb100d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tools(msg: AIMessage):\n",
    "    \"\"\"ÏàúÏ∞®Ï†ÅÏù∏ ÎèÑÍµ¨ Ìò∏Ï∂úÏùÑ ÏúÑÌïú Í∞ÑÎã®Ìïú Ìó¨Ìçº Ìï®ÏàòÏûÖÎãàÎã§.\"\"\"\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    tool_calls = msg.tool_calls.copy()\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1d627c5-3e54-4317-9e81-4a733add0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm_with_tools | call_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d68c9c9f-4d22-45e1-85eb-993be3739d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_current_weather', 'args': {'latitude': 37.5665, 'longitude': 126.978}, 'id': 'call_HA06IeruCldi4MS7uMKM6dy8', 'type': 'tool_call', 'output': '{\"latitude\": 37.55, \"longitude\": 127.0, \"generationtime_ms\": 0.08797645568847656, \"utc_offset_seconds\": 32400, \"timezone\": \"Asia/Seoul\", \"timezone_abbreviation\": \"KST\", \"elevation\": 34.0, \"current_weather_units\": {\"time\": \"iso8601\", \"interval\": \"seconds\", \"temperature\": \"\\\\u00b0C\", \"windspeed\": \"km/h\", \"winddirection\": \"\\\\u00b0\", \"is_day\": \"\", \"weathercode\": \"wmo code\"}, \"current_weather\": {\"time\": \"2024-10-09T15:15\", \"interval\": 900, \"temperature\": 22.5, \"windspeed\": 1.5, \"winddirection\": 135, \"is_day\": 1, \"weathercode\": 2}, \"daily_units\": {\"time\": \"iso8601\", \"temperature_2m_max\": \"\\\\u00b0C\", \"temperature_2m_min\": \"\\\\u00b0C\"}, \"daily\": {\"time\": [\"2024-10-09\", \"2024-10-10\", \"2024-10-11\", \"2024-10-12\", \"2024-10-13\", \"2024-10-14\", \"2024-10-15\"], \"temperature_2m_max\": [22.5, 21.6, 22.5, 23.2, 22.6, 23.1, 24.3], \"temperature_2m_min\": [13.1, 14.4, 12.7, 13.8, 10.9, 14.6, 12.8]}}'}]\n",
      "[{'name': 'get_current_location', 'args': {}, 'id': 'call_MBXInRZPU0JD1W1rFsC0ILMf', 'type': 'tool_call', 'output': '{\"ip\": \"61.79.218.46\", \"city\": \"Bucheon-si\", \"region\": \"Gyeonggi-do\", \"country\": \"KR\", \"loc\": \"37.4989,126.7831\", \"org\": \"AS4766 Korea Telecom\", \"postal\": \"14525\", \"timezone\": \"Asia/Seoul\", \"readme\": \"https://ipinfo.io/missingauth\"}'}]\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is the weather in Seoul?\")\n",
    "print(result)\n",
    "\n",
    "result = chain.invoke(\"What is the current location?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "014b25cb-e9b4-4856-8337-89b69c208c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ÏÑúÏö∏ ÎÇ†Ïî®Î•º ÏïåÎ†§ Ï§ò.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > [{'name': 'get_current_weather', 'args': {'latitude': 37.5665, 'longitude': 126.978}, 'id': 'call_xDIgnHg0IJQkMszrqwSrlniu', 'type': 'tool_call', 'output': '{\"latitude\": 37.55, \"longitude\": 127.0, \"generationtime_ms\": 0.08702278137207031, \"utc_offset_seconds\": 32400, \"timezone\": \"Asia/Seoul\", \"timezone_abbreviation\": \"KST\", \"elevation\": 34.0, \"current_weather_units\": {\"time\": \"iso8601\", \"interval\": \"seconds\", \"temperature\": \"\\\\u00b0C\", \"windspeed\": \"km/h\", \"winddirection\": \"\\\\u00b0\", \"is_day\": \"\", \"weathercode\": \"wmo code\"}, \"current_weather\": {\"time\": \"2024-10-09T15:15\", \"interval\": 900, \"temperature\": 22.5, \"windspeed\": 1.5, \"winddirection\": 135, \"is_day\": 1, \"weathercode\": 2}, \"daily_units\": {\"time\": \"iso8601\", \"temperature_2m_max\": \"\\\\u00b0C\", \"temperature_2m_min\": \"\\\\u00b0C\"}, \"daily\": {\"time\": [\"2024-10-09\", \"2024-10-10\", \"2024-10-11\", \"2024-10-12\", \"2024-10-13\", \"2024-10-14\", \"2024-10-15\"], \"temperature_2m_max\": [22.5, 21.6, 22.5, 23.2, 22.6, 23.1, 24.3], \"temperature_2m_min\": [13.1, 14.4, 12.7, 13.8, 10.9, 14.6, 12.8]}}'}]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "def chat_with_user(user_message):\n",
    "    ai_message = chain.invoke(user_message)\n",
    "    return ai_message\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe7f3b-0498-42f6-a8b9-4b0c57c91f8f",
   "metadata": {},
   "source": [
    "### ÎùºÏö∞ÌåÖ Ï≤¥Ïù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0ac369a-b66b-4b8c-8195-a23763ba81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68652e7e-5775-4b3f-90eb-a0ba2a362f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"ÏïÑÎûòÏùò ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏Ïù¥ Ï£ºÏñ¥Ï°åÏùÑ Îïå, 'LangChain', 'OpenAI', 'Other' Ï§ë Ïñ¥Îäê Í≤ÉÏóê Í¥ÄÌïú ÏßàÎ¨∏Ïù∏ÏßÄ Î∂ÑÎ•òÌïòÏÑ∏Ïöî.\n",
    "\n",
    "Îëê Îã®Ïñ¥ Ïù¥ÏÉÅÏúºÎ°ú ÏùëÎãµÌïòÏßÄ ÎßàÏÑ∏Ïöî.\n",
    "\n",
    "ÏßàÎ¨∏: {question}\n",
    "Î∂ÑÎ•ò:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"question\": \"OpenAIÎ•º Ìò∏Ï∂úÌïòÎ†§Î©¥ Ïñ¥ÎñªÍ≤å ÌïòÎÇòÏöî?\"\n",
    "})\n",
    "print(result)  # Ï∂úÎ†•: OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa90b380-bed9-4c96-be82-14251164ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Í∑ÄÌïòÎäî Îû≠Ï≤¥Ïù∏ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. \n",
    "Ìï≠ÏÉÅ \"Ìï¥Î¶¨Ïä® Ï≤¥Ïù¥Ïä§Í∞Ä ÎßêÌñàÎìØÏù¥\"Î°ú ÏãúÏûëÌïòÎäî ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî. \n",
    "Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî:\n",
    "\n",
    "ÏßàÎ¨∏: {question}\n",
    "ÎãµÎ≥Ä:\"\"\"\n",
    ")  | ChatOpenAI()\n",
    "\n",
    "openai_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Í∑ÄÌïòÎäî OpenAI Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. \n",
    "Ìï≠ÏÉÅ \"ÏÇ¨Î¨¥Ïóò Ìï¥Î¶¨Ïä§ ÏïåÌä∏Î®ºÏù¥ ÎÇòÏóêÍ≤å ÎßêÌñàÎìØÏù¥\"Î°ú ÏãúÏûëÌïòÎäî ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.\n",
    "Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî:\n",
    "\n",
    "ÏßàÎ¨∏: {question}\n",
    "ÎãµÎ≥Ä:\"\"\"\n",
    ") | ChatOpenAI()\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî:\n",
    "\n",
    "ÏßàÎ¨∏: {question}\n",
    "ÎãµÎ≥Ä:\"\"\"\n",
    ") | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4023ef4d-2c81-466b-b700-d27e94a45056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"openai\" in info[\"topic\"].lower():\n",
    "        return openai_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "536048a3-245e-4e1c-9a67-117ce153419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain, \n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92c5cc3c-35ed-44a2-a492-61fd5773208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ÏÇ¨Î¨¥Ïóò Ìï¥Î¶¨Ïä§ ÏïåÌä∏Î®ºÏù¥ ÎÇòÏóêÍ≤å ÎßêÌñàÎìØÏù¥, OpenAIÎäî Ï£ºÎ°ú Ïù∏Í≥µÏßÄÎä• Í∏∞Ïà†ÏùÑ Í∞úÎ∞úÌïòÍ≥† Ïó∞Íµ¨ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§. Ïù¥ Í∏∞Ïà†ÏùÄ Ïñ∏Ïñ¥ Î™®Îç∏, Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±, ÏûêÏú® Ï£ºÌñâ Îì± Îã§ÏñëÌïú Î∂ÑÏïºÏóê Ï†ÅÏö©Îê† Ïàò ÏûàÏäµÎãàÎã§.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 93, 'total_tokens': 189, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5ea778a7-4fb1-4640-aba8-4114e2f9e2ba-0' usage_metadata={'input_tokens': 93, 'output_tokens': 96, 'total_tokens': 189}\n",
      "content='Ìï¥Î¶¨Ïä® Ï≤¥Ïù¥Ïä§Í∞Ä ÎßêÌñàÎìØÏù¥, Îû≠Ï≤¥Ïù∏ÏùÄ Î∏îÎ°ùÏ≤¥Ïù∏ Í∏∞Ïà†ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú Î∂ÑÏÇ∞Ìòï ÎåÄÍ∑úÎ™® ÎÑ§Ìä∏ÏõåÌÅ¨Î°ú, Îç∞Ïù¥ÌÑ∞Ïùò Ìà¨Î™ÖÏÑ±Í≥º ÏïàÏ†ÑÏÑ±ÏùÑ Î≥¥Ïû•ÌïòÎ©∞ Ï§ëÏïô Í∏∞Í¥Ä ÏóÜÏù¥ Ìö®Ïú®Ï†ÅÏù∏ Í±∞Îûò Î∞è Ï†ïÎ≥¥ Í≥µÏú†Î•º Í∞ÄÎä•ÌïòÍ≤å Ìï©ÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ Í∏∞Ï°¥ Ï§ëÏïôÌôîÎêú ÏãúÏä§ÌÖúÍ≥º ÎπÑÍµêÌïòÏó¨ Î≥¥Îã§ ÏïàÏ†ÑÌïòÍ≥† Ïã†ÏÜçÌïú Í±∞ÎûòÍ∞Ä Í∞ÄÎä•Ìï¥ÏßÄÎ©∞, Îû≠Ï≤¥Ïù∏ÏùÑ ÌÜµÌï¥ Îã§ÏñëÌïú Î∂ÑÏïºÏóêÏÑú ÌòÅÏã†Ï†ÅÏù∏ ÏÑúÎπÑÏä§ Î∞è ÎπÑÏ¶àÎãàÏä§ Î™®Îç∏Ïù¥ ÌÉÑÏÉùÌï† Ïàò ÏûàÏäµÎãàÎã§.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 91, 'total_tokens': 286, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d0c74670-ee4e-4576-a0b1-527d65fe2211-0' usage_metadata={'input_tokens': 91, 'output_tokens': 195, 'total_tokens': 286}\n",
      "content='4' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 35, 'total_tokens': 36, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-32f5ea2c-691c-4bb6-b632-440520fa4044-0' usage_metadata={'input_tokens': 35, 'output_tokens': 1, 'total_tokens': 36}\n"
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({\"question\": \"OpenAIÎäî Ïñ¥ÎñªÍ≤å ÏÇ¨Ïö©ÌïòÎÇòÏöî?\"}))\n",
    "print(full_chain.invoke({\"question\": \"Îû≠Ï≤¥Ïù∏ÏùÄ Ïñ¥ÎñªÍ≤å ÏÇ¨Ïö©ÌïòÎÇòÏöî?\"}))\n",
    "print(full_chain.invoke({\"question\": \"2 + 2Îäî?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3aa66552-0842-42c2-aae6-5e8d64db88bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "Î∏îÎûôÌôÄÏùÄ Îß§Ïö∞ Í∞ïÎ†•Ìïú Ï§ëÎ†•ÏùÑ Í∞ÄÏßÑ Ï≤úÏ≤¥Î°ú, Ï∂©Î∂ÑÌûà ÎßéÏùÄ ÏñëÏùò Î¨ºÏßàÏù¥ÎÇò ÏóêÎÑàÏßÄÍ∞Ä ÏûëÏö©ÌïòÏó¨ Ï§ëÏã¨Ïóê Îß§Ïö∞ Î∞ÄÎèÑ ÎÜíÏùÄ ÏßàÎüâÏù¥ Î™®Ïó¨ ÎßåÎì§Ïñ¥ÏßÑ Í≤ÉÏûÖÎãàÎã§. Î∏îÎûôÌôÄÏùò Ï§ëÎ†•Ïû•ÏùÄ Ï£ºÎ≥ÄÏóê ÏûàÎäî Î¨ºÏ≤¥ÎÇò ÎπõÏ°∞Ï∞®ÎèÑ Ìù°ÏàòÌïòÏó¨ ÌÉàÏ∂úÌï† Ïàò ÏóÜÍ≤å ÎßåÎì§Ïñ¥ÏßëÎãàÎã§. Îî∞ÎùºÏÑú Î∏îÎûôÌôÄÏùÄ ÎπõÎßàÏ†ÄÎèÑ Í∞áÏïÑÎ≤ÑÎ¶∞Îã§Í≥† ÏÉùÍ∞ÅÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú ÌäπÏÑ±ÏúºÎ°ú Ïù∏Ìï¥ Î∏îÎûôÌôÄÏùÄ Ïö∞Ï£ºÏóêÏÑú Í∞ÄÏû• Ïù¥ÏÉÅÌïú Ï≤úÏ≤¥ Ï§ë ÌïòÎÇòÎ°ú ÏïåÎ†§Ï†∏ ÏûàÏäµÎãàÎã§.\n",
      "Using MATH\n",
      "Í≤ΩÎ°ú ÌÜµÌï©Ïù¥ÎûÄ Í∑∏ÎûòÌîÑ Ïù¥Î°†ÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Í∞úÎÖêÏúºÎ°ú, Í∑∏ÎûòÌîÑÏóêÏÑú Îëê Ï†ïÏ†ê ÏÇ¨Ïù¥Ïùò Í≤ΩÎ°úÎ•º Ïó∞Í≤∞ÌïòÎäî Í≥ºÏ†ïÏùÑ ÎßêÌï©ÎãàÎã§. Ï¶â, ÌïòÎÇòÏùò Í≤ΩÎ°úÍ∞Ä Ïó¨Îü¨ Í≤ΩÎ°úÎ•º ÌÜµÌï©ÌïòÏó¨ ÏµúÎã® Í≤ΩÎ°ú ÎòêÎäî ÌäπÏ†ï Ï°∞Í±¥ÏùÑ ÎßåÏ°±ÌïòÎäî Í≤ΩÎ°úÎ•º Ï∞æÎäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§. Í≤ΩÎ°ú ÌÜµÌï©ÏùÄ ÎÑ§Ìä∏ÏõåÌÅ¨ ÏµúÏ†ÅÌôî, Îç∞Ïù¥ÌÑ∞ ÌÜµÏã†, ÎùºÏö∞ÌåÖ Îì± Îã§ÏñëÌïú Î∂ÑÏïºÏóêÏÑú ÏùëÏö©ÎêòÎ©∞, Í∑∏ÎûòÌîÑ Ïù¥Î°†Ïùò Ï§ëÏöîÌïú Í∞úÎÖê Ï§ë ÌïòÎÇòÏûÖÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Î¨ºÎ¶¨Ìïô ÏßàÎ¨∏Ïóê ÎåÄÌïú ÌÖúÌîåÎ¶øÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.\n",
    "physics_template = \"\"\"ÎãπÏã†ÏùÄ Îß§Ïö∞ ÎòëÎòëÌïú Î¨ºÎ¶¨Ìïô ÍµêÏàòÏûÖÎãàÎã§. \n",
    "Î¨ºÎ¶¨ÌïôÏóê ÎåÄÌïú ÏßàÎ¨∏Ïóê Í∞ÑÍ≤∞ÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å ÎåÄÎãµÌïòÎäî Îç∞ Îä•ÏàôÌï©ÎãàÎã§. \n",
    "ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÏùÑ Î™®Î•º ÎïåÎäî Î™®Î•∏Îã§Í≥† Ïù∏Ï†ïÌï©ÎãàÎã§.\n",
    "\n",
    "Îã§ÏùåÏùÄ ÏßàÎ¨∏ÏûÖÎãàÎã§:\n",
    "{query}\"\"\"\n",
    "\n",
    "# ÏàòÌïô ÏßàÎ¨∏Ïóê ÎåÄÌïú ÌÖúÌîåÎ¶øÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.\n",
    "math_template = \"\"\"ÎãπÏã†ÏùÄ ÏïÑÏ£º ÌõåÎ•≠Ìïú ÏàòÌïôÏûêÏûÖÎãàÎã§. ÎãπÏã†ÏùÄ ÏàòÌïô Î¨∏Ï†úÏóê ÎåÄÌïú ÎãµÏùÑ ÏûòÌï©ÎãàÎã§. \n",
    "ÎãπÏã†ÏùÄ Ïñ¥Î†§Ïö¥ Î¨∏Ï†úÎ•º Íµ¨ÏÑ± ÏöîÏÜåÎ°ú Î∂ÑÌï¥ÌïòÍ≥†, \"Íµ¨ÏÑ± ÏöîÏÜåÏóê ÎãµÌïú Îã§Ïùå \"Îçî ÎÑìÏùÄ ÏßàÎ¨∏Ïóê ÎãµÌï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Îß§Ïö∞ ÌõåÎ•≠Ìï©ÎãàÎã§.\n",
    "Íµ¨ÏÑ± ÏöîÏÜåÏóê ÎãµÌïú Îã§Ïùå Ïù¥Î•º Ï¢ÖÌï©ÌïòÏó¨ Îçî ÎÑìÏùÄ ÏßàÎ¨∏Ïóê ÎãµÌï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n",
    "\n",
    "Ïó¨Í∏∞ ÏßàÎ¨∏Ïù¥ ÏûàÏäµÎãàÎã§:\n",
    "{query}\"\"\"\n",
    "\n",
    "# OpenAIEmbeddings Í∞ùÏ≤¥Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Î¨ºÎ¶¨ÌïôÍ≥º ÏàòÌïô ÌÖúÌîåÎ¶øÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î¨∂ÏäµÎãàÎã§.\n",
    "prompt_templates = [physics_template, math_template]\n",
    "\n",
    "# ÌÖúÌîåÎ¶øÎì§ÏùÑ ÏûÑÎ≤†Îî©Ìï©ÎãàÎã§.\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "# ÏûÖÎ†•Îêú ÏßàÎ¨∏Ïóê Îî∞Îùº Ï†ÅÏ†àÌïú ÌÖúÌîåÎ¶øÏùÑ ÏÑ†ÌÉùÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "def prompt_router(input):\n",
    "    # ÏûÖÎ†•Îêú ÏßàÎ¨∏ÏùÑ ÏûÑÎ≤†Îî©Ìï©ÎãàÎã§.\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    \n",
    "    # ÏßàÎ¨∏ ÏûÑÎ≤†Îî©Í≥º ÌÖúÌîåÎ¶ø ÏûÑÎ≤†Îî© Í∞ÑÏùò ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    \n",
    "    # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú ÌÖúÌîåÎ¶øÏùÑ ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    \n",
    "    # ÏÑ†ÌÉùÎêú ÌÖúÌîåÎ¶øÏù¥ ÏàòÌïô ÌÖúÌîåÎ¶øÏù∏ÏßÄ Î¨ºÎ¶¨Ìïô ÌÖúÌîåÎ¶øÏù∏ÏßÄ Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "\n",
    "    # ÏÑ†ÌÉùÎêú ÌÖúÌîåÎ¶øÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "# Ï≤¥Ïù∏ÏùÑ Íµ¨ÏÑ±Ìï©ÎãàÎã§.\n",
    "chain = (\n",
    "    { \"query\": RunnablePassthrough() }  # ÏûÖÎ†•Îêú ÏßàÎ¨∏ÏùÑ Í∑∏ÎåÄÎ°ú ÌÜµÍ≥ºÏãúÌÇµÎãàÎã§.\n",
    "    | RunnableLambda(prompt_router)  # ÏßàÎ¨∏Ïóê Îî∞Îùº Ï†ÅÏ†àÌïú ÌÖúÌîåÎ¶øÏùÑ ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
    "    | ChatOpenAI()  # ÏÑ†ÌÉùÎêú ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ OpenAIÏôÄ ÎåÄÌôîÌï©ÎãàÎã§.\n",
    "    | StrOutputParser()  # Í≤∞Í≥ºÎ•º Î¨∏ÏûêÏó¥Î°ú ÌååÏã±Ìï©ÎãàÎã§.\n",
    ")\n",
    "\n",
    "# Ï≤¥Ïù∏ÏùÑ Ïã§ÌñâÌïòÏó¨ ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥ÄÏùÑ Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
    "print(chain.invoke(\"Î∏îÎûôÌôÄÏù¥ÎûÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\"))\n",
    "print(chain.invoke(\"Í≤ΩÎ°ú ÌÜµÌï©Ïù¥ÎûÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a8df6-1749-449f-ba88-6176dd270aee",
   "metadata": {},
   "source": [
    "### 10. RAG-ÎåÄÌïôÍµê ÌïôÏÉù ÏÑ±Ï†ÅÏóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎäî ÏöîÏÜåÎ•º ÌÉêÏÉâÌïòÍ≥† Î∂ÑÏÑùÌïòÎäî ÏãúÏä§ÌÖú Íµ¨Ï∂ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5239853-f9f9-4601-a5ea-ae28cd7d15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÎåÄÌïôÏÉùÏùò ÌïôÏóÖÏÑ±Ï∑®Ïóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎäî Ïã¨Î¶¨Ï†Å, ÌïôÏäµÏó≠Îüâ, ÌôòÍ≤Ω/ÏßÄÏßÄ ÏöîÏù∏ ÌÉêÏÉâ  289\n",
      "2. ÌïôÏóÖÏÑ±Ï∑®ÏôÄ ÌïôÏäµÏó≠Îüâ Î≥ÄÏù∏Í≥ºÏùò Í¥ÄÍ≥Ñ\n",
      "ÎåÄÌïôÏÉùÏùò ÌïôÏóÖÏÑ±Ï∑®Ïóê ÏßÅÏ†ëÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ Ï£ºÎäî ÌïôÏäµÏó≠Îüâ Î≥ÄÏù∏ÏúºÎ°úÎäî ÌïôÏäµÎèôÍ∏∞ÏôÄ ÌïôÏäµÏäµÍ¥ÄÏù¥ Ïûà\n",
      "Îã§. ÎèôÍ∏∞Îäî ÌïôÏäµÍ≥ºÏ†ïÍ≥º Î∞ÄÏ†ëÌïú Í¥ÄÎ†®Ïù¥ ÏûàÎäîÎç∞, ÌïôÏäµÏóê ÏûàÏñ¥ÏÑú ÎèôÍ∏∞Îäî Ïñ¥Îñ§ ÎÇ¥Ïö©ÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, Ïñ∏\n",
      "Ï†ú Í≥µÎ∂ÄÎ•º ÌïòÎ©∞, Ïñ¥Îñ§ Î∞©Î≤ïÏúºÎ°ú Í≥µÎ∂ÄÌïòÎäîÏßÄ Îì± Í∑∏ Í≥ºÏ†ï Ï†ÑÎ∞òÏóê ÏòÅÌñ•ÏùÑ Ï§ÄÎã§(Schunk, 1989). ÌïôÏäµ\n",
      "ÏùÄ ÏùòÏãùÏ†ÅÏù¥Í≥† ÏùòÎèÑÏ†ÅÏù∏ ÌôúÎèôÏù¥ ÏöîÍµ¨ÎêòÎäî Îä•ÎèôÏ†ÅÏù∏ Í≥ºÏ†ïÏù¥ÎØÄÎ°ú Í∞úÏù∏Ïùò ÌôúÎèôÏùÑ Ïú†Î∞úÏãúÌÇ§Í≥† Ïú†ÏßÄ\n",
      "ÌïòÎ©∞, Ïù¥Îü¨Ìïú ÌôúÎèôÏùÑ ÌÜµÌï¥ Ïñ¥Îñ§ Î™©Ìëú\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "#\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader  # PDF Î°úÎçî Ï∂îÍ∞Ä\n",
    "from langchain.chains import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú: Ïô∏Î∂Ä Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµ ÏûêÎ£å Î°úÎìú\n",
    "loader = DirectoryLoader(\n",
    "    './student_grades_research_papers',\n",
    "    glob=\"**/*.pdf\",  # PDF ÌååÏùºÎßå Î°úÎìú\n",
    "    loader_cls=PyMuPDFLoader  # PDF Î°úÎçî ÏÇ¨Ïö©\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Î¨∏ÏÑú ÎÇ¥Ïö© ÌôïÏù∏ (Ïòà: 5Î≤àÏß∏ Î¨∏ÏÑúÏùò Ï≤´ 300Ïûê Ï∂úÎ†•)\n",
    "if len(documents) > 4:\n",
    "    print(documents[4].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7ef1282-2f25-4d51-b1ce-dcf030f86453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Î¨∏ÏÑú ÏûÑÎ≤†Îî©: Î¨∏ÏÑú ÏûÑÎ≤†Îî©ÏùÑ ÏÉùÏÑ±ÌïòÏó¨ Í≤ÄÏÉâÏù¥ Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏÑ§Ï†ï\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 3. Í≤ÄÏÉâ ÏøºÎ¶¨ ÏÑ§Ï†ï\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# 4. LLM Î∞è ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"documents\"],\n",
    "    template=\"Îã§Ïùå Î¨∏ÏÑúÎì§ÏùÑ Î∞îÌÉïÏúºÎ°ú ÎåÄÌïôÍµê ÌïôÏÉùÏùò ÏÑ±Ï†ÅÏóê ÏòÅÌñ•ÏùÑ Ï£ºÎäî ÏöîÏÜåÎì§ÏùÑ ÏÑ§Î™ÖÌï¥ Ï§ò: {documents}\"\n",
    ")\n",
    "\n",
    "# LLMChain ÏÉùÏÑ±\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# StuffDocumentsChain ÏÉùÏÑ±\n",
    "combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain)\n",
    "\n",
    "# 5. Í≤ÄÏÉâÍ≥º ÏÉùÏÑ± Î™®Îç∏ÏùÑ Ïó∞Í≤∞ÌïòÎäî RetrievalQA Íµ¨ÏÑ±\n",
    "qa_chain = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1922857d-fda0-466d-ba32-6443626c8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '\\nÎåÄÌïôÍµê ÌïôÏÉùÏùò ÏÑ±Ï†ÅÏóê ÏòÅÌñ•ÏùÑ Ï£ºÎäî ÏöîÏÜåÎì§ÏùÑ ÏÑ§Î™ÖÌï¥ Ï§ò. \\nÍ≤ÄÏÉâÎêú Ïó∞Íµ¨ ÏûêÎ£åÎ•º Î∞îÌÉïÏúºÎ°ú Î∂ÑÏÑùÌï¥ Ï£ºÍ≥†, ÏÑ±Ï†Å Ìñ•ÏÉÅÏóê ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÎäî Ï†ÑÎûµÏùÑ Ï†úÏãúÌï¥ Ï§ò.\\n', 'result': 'ÎåÄÌïôÏÉùÏùò ÏÑ±Ï†ÅÏóê ÏòÅÌñ•ÏùÑ Ï£ºÎäî ÏöîÏÜåÎì§ÏùÄ Ïó¨Îü¨ Í∞ÄÏßÄÎ°ú ÎÇòÎàå Ïàò ÏûàÏúºÎ©∞, Ï£ºÎ°ú Ïã¨Î¶¨Ï†Å ÏöîÏù∏, ÌïôÏäµÏó≠Îüâ, ÌôòÍ≤Ω Î∞è ÏßÄÏßÄ ÏöîÏù∏ÏúºÎ°ú Íµ¨Î∂ÑÌï† Ïàò ÏûàÏäµÎãàÎã§. Îã§ÏùåÏùÄ Í∞Å ÏöîÏÜåÏóê ÎåÄÌïú ÏÑ§Î™ÖÏûÖÎãàÎã§.\\n\\n### 1. Ïã¨Î¶¨Ï†Å ÏöîÏù∏\\n- **ÌïôÏóÖÏ†Å ÏûêÍ∏∞Ìö®Îä•Í∞ê**: ÌïôÏÉùÏù¥ ÏûêÏã†Ïùò ÌïôÏóÖ Îä•Î†•Ïóê ÎåÄÌï¥ Í∞ÄÏßÄÎäî Ïã†ÎÖêÏúºÎ°ú, ÎÜíÏùÄ ÏûêÍ∏∞Ìö®Îä•Í∞êÏùÄ ÌïôÏóÖ ÏÑ±Ï∑®Ïóê Í∏çÏ†ïÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ©ÎãàÎã§. ÏûêÏã†Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞Ìï† Ïàò ÏûàÎã§Îäî ÎØøÏùåÏùÄ ÎèÑÏ†ÑÏ†ÅÏù∏ ÏÉÅÌô©ÏóêÏÑúÎèÑ ÏßÄÏÜçÏ†ÅÏúºÎ°ú ÎÖ∏Î†•ÌïòÍ≤å ÎßåÎì≠ÎãàÎã§.\\n- **ÏÑ±Ïã§ÏÑ±**: ÌïôÏóÖÏóê ÎåÄÌïú Íæ∏Ï§ÄÌïú ÎÖ∏Î†•Í≥º Ï±ÖÏûÑÍ∞êÏùÄ ÏÑ±Ï†Å Ìñ•ÏÉÅÏóê Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§. ÏÑ±Ïã§Ìïú ÌïôÏÉùÏùÄ ÌïôÏäµÏóê Îçî ÎßéÏùÄ ÏãúÍ∞ÑÏùÑ Ìà¨ÏûêÌïòÍ≥†, Í≥ºÏ†úÎ•º ÏÑ±Ïã§Ìûà ÏàòÌñâÌïòÎäî Í≤ΩÌñ•Ïù¥ ÏûàÏäµÎãàÎã§.\\n- **ÌïôÏäµÎèôÍ∏∞**: ÌïôÏäµÏóê ÎåÄÌïú ÎÇ¥Ï†Å ÎèôÍ∏∞ÏôÄ Ïô∏Ï†Å ÎèôÍ∏∞Îäî ÌïôÏÉùÏùò ÌïôÏóÖ ÏÑ±Ï∑®Ïóê ÌÅ∞ ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ©ÎãàÎã§. ÎÜíÏùÄ ÌïôÏäµÎèôÍ∏∞Î•º Í∞ÄÏßÑ ÌïôÏÉùÏùÄ Îçî ÎßéÏùÄ ÎÖ∏Î†•ÏùÑ Í∏∞Ïö∏Ïù¥Í≥†, ÌïôÏäµÏóê ÎåÄÌïú Ìù•ÎØ∏Î•º Ïú†ÏßÄÌï©ÎãàÎã§.\\n\\n### 2. ÌïôÏäµÏó≠Îüâ\\n- **ÌïôÏäµÏäµÍ¥Ä**: Ìö®Í≥ºÏ†ÅÏù∏ ÌïôÏäµÏäµÍ¥ÄÏùÄ ÏÑ±Ï†Å Ìñ•ÏÉÅÏóê Í∏∞Ïó¨Ìï©ÎãàÎã§. Ï†ïÍ∏∞Ï†ÅÏù∏ Î≥µÏäµ, ÏãúÍ∞Ñ Í¥ÄÎ¶¨, Î™©Ìëú ÏÑ§Ï†ï Îì±ÏùÄ ÌïôÏäµÏùò ÏßàÏùÑ ÎÜíÏù¥Îäî Îç∞ ÎèÑÏõÄÏù¥ Îê©ÎãàÎã§.\\n- **ÏûêÍ∏∞Ï°∞Ï†àÌïôÏäµ**: ÏûêÏã†Ïùò ÌïôÏäµ Í≥ºÏ†ïÏùÑ Í≥ÑÌöçÌïòÍ≥† Ï°∞Ï†àÌïòÎäî Îä•Î†•ÏùÄ ÏÑ±Ï†ÅÏóê Í∏çÏ†ïÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ©ÎãàÎã§. ÏûêÍ∏∞Ï°∞Ï†àÌïôÏäµÏù¥ Ïûò Ïù¥Î£®Ïñ¥ÏßÄÎäî ÌïôÏÉùÏùÄ ÌïôÏäµ Î™©ÌëúÎ•º ÏÑ§Ï†ïÌïòÍ≥†, Í∑∏Ïóê ÎßûÏ∂∞ ÌïôÏäµ Ï†ÑÎûµÏùÑ Ï°∞Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§.\\n\\n### 3. ÌôòÍ≤Ω Î∞è ÏßÄÏßÄ ÏöîÏù∏\\n- **ÍµêÏàòÏßÄÏßÄ**: ÍµêÏàòÏôÄÏùò Í∏çÏ†ïÏ†ÅÏù∏ Í¥ÄÍ≥ÑÎäî ÌïôÏÉùÏùò ÌïôÏóÖ ÏÑ±Ï∑®Ïóê ÌÅ∞ ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ©ÎãàÎã§. ÍµêÏàòÏùò ÌîºÎìúÎ∞±Í≥º ÏßÄÏõêÏùÄ ÌïôÏÉùÏù¥ ÌïôÏäµÏóê ÎåÄÌïú ÏûêÏã†Í∞êÏùÑ Í∞ñÍ≤å ÌïòÍ≥†, Î¨∏Ï†ú Ìï¥Í≤∞Ïóê ÎèÑÏõÄÏùÑ Ï§çÎãàÎã§.\\n- **ÌïôÏäµÌôòÍ≤Ω**: Î¨ºÎ¶¨Ï†Å ÌôòÍ≤Ω(Ïòà: ÎèÑÏÑúÍ¥Ä, Í∞ïÏùòÏã§)Í≥º ÏÇ¨ÌöåÏ†Å ÌôòÍ≤Ω(Ïòà: ÏπúÍµ¨, Í∞ÄÏ°±)Ïùò ÏßàÏùÄ ÌïôÏäµÏóê ÏòÅÌñ•ÏùÑ ÎØ∏Ïπ©ÎãàÎã§. ÏßÄÏõêÏ†ÅÏù∏ ÌïôÏäµ ÌôòÍ≤ΩÏùÄ ÌïôÏÉùÏù¥ Îçî ÎÇòÏùÄ ÏÑ±Ï†ÅÏùÑ ÏñªÎèÑÎ°ù ÎèÑÏôÄÏ§çÎãàÎã§.\\n- **ÏÇ¨ÌöåÏ†Å ÏßÄÏßÄ**: ÏπúÍµ¨, Í∞ÄÏ°±, ÎèôÎ£åÎ°úÎ∂ÄÌÑ∞Ïùò Ï†ïÏÑúÏ†Å Î∞è Ïã§ÏßàÏ†ÅÏù∏ ÏßÄÏõêÏùÄ ÌïôÏÉùÏùò Ïä§Ìä∏Î†àÏä§Î•º Ï§ÑÏù¥Í≥†, ÌïôÏóÖÏóê ÎåÄÌïú Í∏çÏ†ïÏ†ÅÏù∏ ÌÉúÎèÑÎ•º Ïú†ÏßÄÌïòÎäî Îç∞ Í∏∞Ïó¨Ìï©ÎãàÎã§.\\n\\n### Í≤∞Î°†\\nÎåÄÌïôÏÉùÏùò ÏÑ±Ï†ÅÏùÄ Ïã¨Î¶¨Ï†Å ÏöîÏù∏, ÌïôÏäµÏó≠Îüâ, ÌôòÍ≤Ω Î∞è ÏßÄÏßÄ ÏöîÏù∏ Îì± Îã§ÏñëÌïú ÏöîÏÜåÏùò ÏÉÅÌò∏ÏûëÏö©Ïóê ÏùòÌï¥ Í≤∞Ï†ïÎê©ÎãàÎã§. Îî∞ÎùºÏÑú, ÌïôÏÉù Í∞úÍ∞úÏù∏Ïùò ÌäπÏÑ±Í≥º ÏÉÅÌô©Ïóê ÎßûÏ∂ò ÎßûÏ∂§Ìòï ÏßÄÏõêÏù¥ ÌïÑÏöîÌïòÎ©∞, Ïù¥Îü¨Ìïú ÏöîÏÜåÎì§ÏùÑ Ï¢ÖÌï©Ï†ÅÏúºÎ°ú Í≥†Î†§Ìïú Ïó∞Íµ¨ÏôÄ ÌîÑÎ°úÍ∑∏Îû® Í∞úÎ∞úÏù¥ Ï§ëÏöîÌï©ÎãàÎã§.'}\n"
     ]
    }
   ],
   "source": [
    "# 6. Prompt ÏÑ§Ï†ï\n",
    "prompt = \"\"\"\n",
    "ÎåÄÌïôÍµê ÌïôÏÉùÏùò ÏÑ±Ï†ÅÏóê ÏòÅÌñ•ÏùÑ Ï£ºÎäî ÏöîÏÜåÎì§ÏùÑ ÏÑ§Î™ÖÌï¥ Ï§ò. \n",
    "Í≤ÄÏÉâÎêú Ïó∞Íµ¨ ÏûêÎ£åÎ•º Î∞îÌÉïÏúºÎ°ú Î∂ÑÏÑùÌï¥ Ï£ºÍ≥†, ÏÑ±Ï†Å Ìñ•ÏÉÅÏóê ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÎäî Ï†ÑÎûµÏùÑ Ï†úÏãúÌï¥ Ï§ò.\n",
    "\"\"\"\n",
    "\n",
    "# 7. ÏßàÎ¨∏ Ïã§Ìñâ\n",
    "try:\n",
    "    # Í≤ÄÏÉâÎêú Î¨∏ÏÑú Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "    retrieved_documents = retriever.get_relevant_documents(prompt)  # promptÎ°ú Í≤ÄÏÉâ Ïã§Ìñâ\n",
    "    \n",
    "    # Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò\n",
    "    if not retrieved_documents:\n",
    "        raise ValueError(\"Í≤ÄÏÉâÎêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "    \n",
    "    # Í∞Å Î¨∏ÏÑúÏùò ÎÇ¥Ïö©ÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò\n",
    "    documents_text = \"\\n\".join(\n",
    "        [doc.page_content for doc in retrieved_documents if isinstance(doc.page_content, str) and doc.page_content]\n",
    "    )  # ÎπÑÏñ¥ÏûàÏßÄ ÏïäÏùÄ Î¨∏ÏÑú ÎÇ¥Ïö©Îßå Ìè¨Ìï®\n",
    "    \n",
    "    if not documents_text.strip():  # Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ ÎπÑÏñ¥ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "        raise ValueError(\"Î¨∏ÏÑú ÎÇ¥Ïö©Ïù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.\")\n",
    "    \n",
    "    # ÏßàÎ¨∏ Ïã§Ìñâ\n",
    "    result = qa_chain.invoke({\"query\": prompt})  # promptÎ°ú query Ï†ÑÎã¨\n",
    "    print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ÏßàÎ¨∏ Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631dda7-b30d-40c6-8ce2-edbc192ada4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_langchain",
   "language": "python",
   "name": "py310_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
