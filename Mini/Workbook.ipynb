{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63a37df-24c9-418e-bfa4-dd7d917c06d7",
   "metadata": {},
   "source": [
    "# ì›Œí¬-ë¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b1daf-0900-4a1d-8e6e-44578e946cbd",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d1d9b2-ec63-4c0d-ad40-96e212637461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 15:02:12.707027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552bfce-83d2-43ba-aa46-9de9b3169f81",
   "metadata": {},
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f3a0ad-0d8c-4a7e-8e87-f021d6a5939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic' # Windows\n",
    "# matplotlib.rcParams['font.family'] = 'AppleGothic' # Mac\n",
    "matplotlib.rcParams['font.size'] = 15 # ê¸€ì í¬ê¸°\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # í•œê¸€ í°íŠ¸ ì‚¬ìš© ì‹œ, ë§ˆì´ë„ˆìŠ¤ ê¸€ìê°€ ê¹¨ì§€ëŠ” í˜„ìƒì„ í•´ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796f5fdc-de25-449f-ae54-83a73271b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.5 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc250fdb-84d7-4c76-8607-81ccf9231391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "3.5.0\n",
      "1.26.4\n",
      "2.1.4\n",
      "3.10.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "\n",
    "# this prints the library version\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "\n",
    "# this prints the python version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d20dbc-56c5-4b1e-bbe8-5eca06665359",
   "metadata": {},
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47edc5d5-941b-42d4-b795-f2dbd442c1b8",
   "metadata": {},
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8586393-6e48-4a44-8f41-96a48fdf37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì¤€ë¹„\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<OpenAI_APIì˜ API í‚¤>\"\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7886278-8cb7-429b-ac44-59683aeccbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d129c61d-307f-4ffd-bfce-2ab35cf718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6c4fd-66bd-4c95-8261-204d9d9b4012",
   "metadata": {},
   "source": [
    "## ã€1ã€ ë­ì²´ì¸ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262d266-b019-4d64-bc79-bd52c1ddce2b",
   "metadata": {},
   "source": [
    "### ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ì²´ì¸ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21a88d3-1bca-4b16-8cd2-b0fd79ff8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25639181-b20d-4c20-8952-e4d414900895",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "chain = prompt_template | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a200cd-eb90-4fcd-8b41-e3690a338141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'topic': 'cat'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdda4c-d024-479f-9768-ab3d4b49d4fd",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ê°ì„± ë¶„ë¥˜ ì²´ì¸ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64a9352-2969-4307-bd51-11410a0c0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de55cbea-c159-4836-8f40-7358b75ede81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(Enum):\n",
    "    POSITIVE = 'positive'\n",
    "    NEGATIVE = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b15b6496-9841-4821-9e5d-dd4c0db6413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\"Classify '{input_text}' as 'positive' or 'negative' only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2e5875-728d-4c97-a6d1-820b54cc2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | ChatOpenAI() | EnumOutputParser(enum=Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b31ff7-eae6-40d1-b6d7-060113bdb3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "Sentiment.NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'input_text':'I love LLM.'})\n",
    "print(result) \n",
    "\n",
    "result = chain.invoke({'input_text':'I hate LLM.'})\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "505c1719-6059-4c85-bc1c-83e8a93b8809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "Sentiment.NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'input_text':'I love and hate LLM.'})\n",
    "print(result) \n",
    "\n",
    "result = chain.invoke({'input_text':'I hate and love LLM.'})\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c496eb-ec75-40f5-919d-922b5f3422ad",
   "metadata": {},
   "source": [
    "### ê°„ë‹¨í•œ ì±—ë´‡ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64aee1a5-7091-42f0-9655-d0d68ba8af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e98fd3c-63e9-4687-adb2-9112a6211d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Talk like a close friend and use emojis ğŸ˜Š\"),\n",
    "    (\"human\", \"{user_input}\")]\n",
    ")\n",
    "\n",
    "chain = {\"user_input\": RunnablePassthrough()} | prompt_template | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4480dc1a-89d2-4fe8-8550-764a708c1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ì•ˆë…•í•˜ì„¸ìš”?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > ì•ˆë…•~ ì–´ë–»ê²Œ ì§€ë‚´? ğŸ˜Š ì˜¤ëœë§Œì´ì•¼~ ë­ í•´?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "        \n",
    "    print(\" A I > \", end=\"\", flush=True)\n",
    "    for chunk in chain.stream(user_message):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa84e5-aabf-490c-8678-243b18c7d5ce",
   "metadata": {},
   "source": [
    "### ìˆ˜í•™ ì—°ì‚° ì²´ì¸ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc990b-d044-4e60-a548-44eb87060a95",
   "metadata": {},
   "source": [
    "pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e237fb0-2f0c-4645-af43-2faa58a30ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c372a35-3022-4bed-81f2-960a0e05282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"For {query}, write only the mathematical expression suitable for numexpr.evaluate().\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4da895-d05e-4121-b50c-62ded4a2a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | ChatOpenAI() | StrOutputParser() | numexpr.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b19fe60-395d-465a-947f-b5c9b3175e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'query': '4ì™€ 5ë¥¼ ë”í•´ì¤˜.'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915df925-deba-40cb-952a-847ada9043c3",
   "metadata": {},
   "source": [
    "### PythonREPLì„ í™œìš©í•œ ë™ì  ì½”ë“œ ì‹¤í–‰ ì±—ë´‡ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11da63fd-d08f-401b-b73c-f22d561c303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcadfd5c-352c-4f63-814f-a72dc84c509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "ì‚¬ìš©ìì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "ì‚¬ìš©ìì—ê²Œ í™•ì¸ì´ë‚˜ ì¶”ê°€ ì •ë³´ë¥¼ ë‹¤ì‹œ ë¬»ì§€ ë§ˆì„¸ìš”.\n",
    "ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” íŒŒì´ì¬ ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template),\n",
    "     (\"human\", \"{user_input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9f94ae8-b052-4ed9-a394-7130d4478539",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"user_input\": RunnablePassthrough()} | prompt_template | ChatOpenAI() | StrOutputParser() | PythonREPL().run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9e7279a-795f-4384-a5f0-2c3cf948f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_user(user_message):\n",
    "    max_attempts = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            ai_message = chain.invoke(user_message)\n",
    "            if \"error\" in ai_message.lower() or \"failed\" in ai_message.lower():\n",
    "                raise Exception(f\"Detected error in AI message: {ai_message}\")\n",
    "            return ai_message\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            print(f\"Attempt {attempts}: Error - {e}\")\n",
    "            if attempts == max_attempts:\n",
    "                return \"Sorry, I am unable to process your request at the moment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4835d532-a6db-430f-96f6-7ce25179557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ì•ˆë…•!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: Error - Detected error in AI message: SyntaxError('invalid syntax', ('<string>', 1, 6, 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\\n', 1, 7))\n",
      "Attempt 2: Error - Detected error in AI message: SyntaxError('invalid syntax', ('<string>', 1, 6, 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\\n', 1, 7))\n",
      "Attempt 3: Error - Detected error in AI message: SyntaxError('invalid syntax', ('<string>', 1, 6, 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\\n', 1, 7))\n",
      " A I > Sorry, I am unable to process your request at the moment.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  í•œê¸€ë‚ ì„ ì•Œë ¤ ì¤˜\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > í•œê¸€ë‚ ì€ 10ì›” 9ì¼ì…ë‹ˆë‹¤.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd0164-aef0-4485-9eaf-e54620022919",
   "metadata": {},
   "source": [
    "### RAG ì²´ì¸ êµ¬ì„±-PDF ë¬¸ì„œ ê¸°ë°˜ QAì±—ë´‡ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac9d7f0-68b1-402f-9a70-fbe0973e2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# PDF íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "folder_path = './datasets/pdfs/'  \n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "texts = []\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•  ë•Œ ì‚¬ìš©í•  CharacterTextSplitter ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\", # í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•  ë•Œ ì‚¬ìš©í•  êµ¬ë¶„ì\n",
    "    chunk_size = 1000, # ê° ë¶„í• ëœ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "    chunk_overlap = 50) # ë¶„í• ëœ í…ìŠ¤íŠ¸ ê°„ì˜ ì¤‘ì²© ê¸¸ì´\n",
    "\n",
    "# ì§€ì •ëœ í´ë” ë‚´ì˜ ëª¨ë“  íŒŒì¼ì„ ìˆœíšŒí•©ë‹ˆë‹¤.\n",
    "for filename in os.listdir(folder_path):\n",
    "    # íŒŒì¼ì´ PDF í˜•ì‹ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    if filename.endswith(\".pdf\"): \n",
    "        # PDF íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ raw_documentsì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        raw_documents = PyPDFLoader(folder_path + '/' + filename).load()\n",
    "        # ë¡œë“œëœ ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ documentsì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "        # ë¶„í• ëœ ë¬¸ì„œë¥¼ texts ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        texts.extend(documents)\n",
    "\n",
    "# ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ Embeddingsë¡œ ë³€í™˜í•˜ì—¬ Chroma ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings())\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” retriever ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# search_kwargs: ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ (ì—¬ê¸°ì„œëŠ” ìƒìœ„ 10ê°œì˜ ê²°ê³¼ë¥¼ ë°˜í™˜)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08059c7a-03ab-41da-8872-77e46662efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"ë‹¹ì‹ ì€ ì§ˆë¬¸ ë‹µë³€ ì‘ì—…ì˜ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ë‹µë³€ì€ ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³  ë©”íƒ€ë°ì´í„° ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. \\ns\\nì§ˆë¬¸: {question} \\në¬¸ë§¥: {context} \\në‹µë³€:\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ade58fa7-43cd-42da-916e-aee7d60befbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  LLM ëª¨ë¸ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > LLMì€ Large Language Modelsì˜ ì•½ìë¡œ, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ê±°ëŒ€í•œ ì–¸ì–´ ëª¨ë¸ì„ ê°€ë¦¬í‚µë‹ˆë‹¤. LLMì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ëŒ€í•œ ì§€ì‹ì„ í¬ê´„ì ìœ¼ë¡œ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°, ì§€ì‹ ì§‘ì•½ì ì¸ ì‘ì—…ì´ë‚˜ ìì—°ì–´ ì´í•´, ìƒì„± ì‘ì—… ë“±ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° ì •ë³´: í˜ì´ì§€ 1, ì†ŒìŠ¤: './datasets/pdfs//Harnessing the Power of LLMs in Practice A Survey on ChatGPT and Beyond-2304.13712v2.pdf'.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "def chat_with_user(user_message):\n",
    "    ai_message = chain.invoke(user_message)\n",
    "    return ai_message\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018778b7-5aed-4c62-b5b6-c070af733d0e",
   "metadata": {},
   "source": [
    "### Memory-ëŒ€í™”í˜• ì±—ë´‡ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bd576d-71c7-42a4-a125-49980f420589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feff4bdc-fe55-40d4-a294-dddc7759ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5867/2204816263.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplateì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ì˜ ê¸°ë³¸ í…œí”Œë¦¿ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Talk like a close friend and use emojis ğŸ˜Š\"),  # ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì¹œê·¼í•œ ì¹œêµ¬ì²˜ëŸ¼ ë§í•˜ê³  ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # ëŒ€í™” ê¸°ë¡ì„ ìœ„í•œ ìë¦¬ í‘œì‹œì\n",
    "    (\"human\", \"{user_input}\")  # ì‚¬ìš©ìì˜ ì…ë ¥ì„ í¬í•¨\n",
    "])\n",
    "\n",
    "# ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ìµœê·¼ 3ê°œì˜ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "\n",
    "# ëŒ€í™” ì²´ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "chain = (\n",
    "    { \"user_input\": RunnablePassthrough() }  # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "    | RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )  # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¶ˆëŸ¬ì™€ chat_history ë³€ìˆ˜ì— í• ë‹¹\n",
    "    | prompt_template  # ì„¤ì •í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©\n",
    "    | ChatOpenAI()  # OpenAIì˜ ì±—ë´‡ ëª¨ë¸ì„ ì‚¬ìš©\n",
    "    | StrOutputParser()  # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59076c58-68be-41a5-a310-56811a2610ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ì•ˆë…•?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > ì•ˆë…•~ ğŸ˜Š ì–´ë–»ê²Œ ì§€ë‚´? ë­ í•˜ê³  ìˆì–´?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "def chat_with_user(user_message):\n",
    "    ai_message = chain.invoke(user_message)\n",
    "    memory.save_context({\"input\": user_message}, {\"output\": ai_message})\n",
    "    return ai_message\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be695ceb-80a9-40c1-b3da-0e32a8beb097",
   "metadata": {},
   "source": [
    "### ë„êµ¬ ì²´ì¸ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb495ab5-8bc0-4daa-b73d-d42365f05268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c548a7ae-21b7-48b5-b5b6-e1240ddfc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_weather(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"\n",
    "    Open-Meteo APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ìœ„ë„ì™€ ê²½ë„ì˜ í˜„ì¬ ë‚ ì”¨ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current_weather\": True,\n",
    "        \"daily\": \"temperature_2m_max,temperature_2m_min\",\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    info = {}\n",
    "    if response.status_code == 200:\n",
    "        info = response.json()\n",
    "    return json.dumps(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca60dd7f-8e94-429b-b136-91f3c5d48b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_location() -> str:\n",
    "    \"\"\"\n",
    "    IPinfo.io APIë¥¼ ì‚¬ìš©í•˜ì—¬ IP ì£¼ì†Œ ê¸°ë°˜ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    response = requests.get('https://ipinfo.io')\n",
    "    info = response.json()\n",
    "    return json.dumps(info)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "tools = [get_current_weather, get_current_location]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22554554-9e8e-4a53-805e-719fb100d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tools(msg: AIMessage):\n",
    "    \"\"\"ìˆœì°¨ì ì¸ ë„êµ¬ í˜¸ì¶œì„ ìœ„í•œ ê°„ë‹¨í•œ í—¬í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.\"\"\"\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    tool_calls = msg.tool_calls.copy()\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1d627c5-3e54-4317-9e81-4a733add0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm_with_tools | call_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d68c9c9f-4d22-45e1-85eb-993be3739d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_current_weather', 'args': {'latitude': 37.5665, 'longitude': 126.978}, 'id': 'call_HA06IeruCldi4MS7uMKM6dy8', 'type': 'tool_call', 'output': '{\"latitude\": 37.55, \"longitude\": 127.0, \"generationtime_ms\": 0.08797645568847656, \"utc_offset_seconds\": 32400, \"timezone\": \"Asia/Seoul\", \"timezone_abbreviation\": \"KST\", \"elevation\": 34.0, \"current_weather_units\": {\"time\": \"iso8601\", \"interval\": \"seconds\", \"temperature\": \"\\\\u00b0C\", \"windspeed\": \"km/h\", \"winddirection\": \"\\\\u00b0\", \"is_day\": \"\", \"weathercode\": \"wmo code\"}, \"current_weather\": {\"time\": \"2024-10-09T15:15\", \"interval\": 900, \"temperature\": 22.5, \"windspeed\": 1.5, \"winddirection\": 135, \"is_day\": 1, \"weathercode\": 2}, \"daily_units\": {\"time\": \"iso8601\", \"temperature_2m_max\": \"\\\\u00b0C\", \"temperature_2m_min\": \"\\\\u00b0C\"}, \"daily\": {\"time\": [\"2024-10-09\", \"2024-10-10\", \"2024-10-11\", \"2024-10-12\", \"2024-10-13\", \"2024-10-14\", \"2024-10-15\"], \"temperature_2m_max\": [22.5, 21.6, 22.5, 23.2, 22.6, 23.1, 24.3], \"temperature_2m_min\": [13.1, 14.4, 12.7, 13.8, 10.9, 14.6, 12.8]}}'}]\n",
      "[{'name': 'get_current_location', 'args': {}, 'id': 'call_MBXInRZPU0JD1W1rFsC0ILMf', 'type': 'tool_call', 'output': '{\"ip\": \"61.79.218.46\", \"city\": \"Bucheon-si\", \"region\": \"Gyeonggi-do\", \"country\": \"KR\", \"loc\": \"37.4989,126.7831\", \"org\": \"AS4766 Korea Telecom\", \"postal\": \"14525\", \"timezone\": \"Asia/Seoul\", \"readme\": \"https://ipinfo.io/missingauth\"}'}]\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is the weather in Seoul?\")\n",
    "print(result)\n",
    "\n",
    "result = chain.invoke(\"What is the current location?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "014b25cb-e9b4-4856-8337-89b69c208c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  ì„œìš¸ ë‚ ì”¨ë¥¼ ì•Œë ¤ ì¤˜.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A I > [{'name': 'get_current_weather', 'args': {'latitude': 37.5665, 'longitude': 126.978}, 'id': 'call_xDIgnHg0IJQkMszrqwSrlniu', 'type': 'tool_call', 'output': '{\"latitude\": 37.55, \"longitude\": 127.0, \"generationtime_ms\": 0.08702278137207031, \"utc_offset_seconds\": 32400, \"timezone\": \"Asia/Seoul\", \"timezone_abbreviation\": \"KST\", \"elevation\": 34.0, \"current_weather_units\": {\"time\": \"iso8601\", \"interval\": \"seconds\", \"temperature\": \"\\\\u00b0C\", \"windspeed\": \"km/h\", \"winddirection\": \"\\\\u00b0\", \"is_day\": \"\", \"weathercode\": \"wmo code\"}, \"current_weather\": {\"time\": \"2024-10-09T15:15\", \"interval\": 900, \"temperature\": 22.5, \"windspeed\": 1.5, \"winddirection\": 135, \"is_day\": 1, \"weathercode\": 2}, \"daily_units\": {\"time\": \"iso8601\", \"temperature_2m_max\": \"\\\\u00b0C\", \"temperature_2m_min\": \"\\\\u00b0C\"}, \"daily\": {\"time\": [\"2024-10-09\", \"2024-10-10\", \"2024-10-11\", \"2024-10-12\", \"2024-10-13\", \"2024-10-14\", \"2024-10-15\"], \"temperature_2m_max\": [22.5, 21.6, 22.5, 23.2, 22.6, 23.1, 24.3], \"temperature_2m_min\": [13.1, 14.4, 12.7, 13.8, 10.9, 14.6, 12.8]}}'}]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER >  quit\n"
     ]
    }
   ],
   "source": [
    "def chat_with_user(user_message):\n",
    "    ai_message = chain.invoke(user_message)\n",
    "    return ai_message\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"USER > \")\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    ai_message = chat_with_user(user_message)\n",
    "    print(f\" A I > {ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe7f3b-0498-42f6-a8b9-4b0c57c91f8f",
   "metadata": {},
   "source": [
    "### ë¼ìš°íŒ… ì²´ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0ac369a-b66b-4b8c-8195-a23763ba81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68652e7e-5775-4b3f-90eb-a0ba2a362f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"ì•„ë˜ì˜ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, 'LangChain', 'OpenAI', 'Other' ì¤‘ ì–´ëŠ ê²ƒì— ê´€í•œ ì§ˆë¬¸ì¸ì§€ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
    "\n",
    "ë‘ ë‹¨ì–´ ì´ìƒìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë¶„ë¥˜:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"question\": \"OpenAIë¥¼ í˜¸ì¶œí•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?\"\n",
    "})\n",
    "print(result)  # ì¶œë ¥: OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa90b380-bed9-4c96-be82-14251164ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = PromptTemplate.from_template(\n",
    "    \"\"\"ê·€í•˜ëŠ” ë­ì²´ì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "í•­ìƒ \"í•´ë¦¬ìŠ¨ ì²´ì´ìŠ¤ê°€ ë§í–ˆë“¯ì´\"ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. \n",
    "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€:\"\"\"\n",
    ")  | ChatOpenAI()\n",
    "\n",
    "openai_chain = PromptTemplate.from_template(\n",
    "    \"\"\"ê·€í•˜ëŠ” OpenAI ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "í•­ìƒ \"ì‚¬ë¬´ì—˜ í•´ë¦¬ìŠ¤ ì•ŒíŠ¸ë¨¼ì´ ë‚˜ì—ê²Œ ë§í–ˆë“¯ì´\"ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€:\"\"\"\n",
    ") | ChatOpenAI()\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€:\"\"\"\n",
    ") | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4023ef4d-2c81-466b-b700-d27e94a45056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"openai\" in info[\"topic\"].lower():\n",
    "        return openai_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "536048a3-245e-4e1c-9a67-117ce153419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain, \n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92c5cc3c-35ed-44a2-a492-61fd5773208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ì‚¬ë¬´ì—˜ í•´ë¦¬ìŠ¤ ì•ŒíŠ¸ë¨¼ì´ ë‚˜ì—ê²Œ ë§í–ˆë“¯ì´, OpenAIëŠ” ì£¼ë¡œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ ê°œë°œí•˜ê³  ì—°êµ¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ì–¸ì–´ ëª¨ë¸, ì´ë¯¸ì§€ ìƒì„±, ììœ¨ ì£¼í–‰ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 93, 'total_tokens': 189, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5ea778a7-4fb1-4640-aba8-4114e2f9e2ba-0' usage_metadata={'input_tokens': 93, 'output_tokens': 96, 'total_tokens': 189}\n",
      "content='í•´ë¦¬ìŠ¨ ì²´ì´ìŠ¤ê°€ ë§í–ˆë“¯ì´, ë­ì²´ì¸ì€ ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¶„ì‚°í˜• ëŒ€ê·œëª¨ ë„¤íŠ¸ì›Œí¬ë¡œ, ë°ì´í„°ì˜ íˆ¬ëª…ì„±ê³¼ ì•ˆì „ì„±ì„ ë³´ì¥í•˜ë©° ì¤‘ì•™ ê¸°ê´€ ì—†ì´ íš¨ìœ¨ì ì¸ ê±°ë˜ ë° ì •ë³´ ê³µìœ ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì¡´ ì¤‘ì•™í™”ëœ ì‹œìŠ¤í…œê³¼ ë¹„êµí•˜ì—¬ ë³´ë‹¤ ì•ˆì „í•˜ê³  ì‹ ì†í•œ ê±°ë˜ê°€ ê°€ëŠ¥í•´ì§€ë©°, ë­ì²´ì¸ì„ í†µí•´ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ì„œë¹„ìŠ¤ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì´ íƒ„ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 91, 'total_tokens': 286, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d0c74670-ee4e-4576-a0b1-527d65fe2211-0' usage_metadata={'input_tokens': 91, 'output_tokens': 195, 'total_tokens': 286}\n",
      "content='4' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 35, 'total_tokens': 36, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-32f5ea2c-691c-4bb6-b632-440520fa4044-0' usage_metadata={'input_tokens': 35, 'output_tokens': 1, 'total_tokens': 36}\n"
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({\"question\": \"OpenAIëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?\"}))\n",
    "print(full_chain.invoke({\"question\": \"ë­ì²´ì¸ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?\"}))\n",
    "print(full_chain.invoke({\"question\": \"2 + 2ëŠ”?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3aa66552-0842-42c2-aae6-5e8d64db88bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "ë¸”ë™í™€ì€ ë§¤ìš° ê°•ë ¥í•œ ì¤‘ë ¥ì„ ê°€ì§„ ì²œì²´ë¡œ, ì¶©ë¶„íˆ ë§ì€ ì–‘ì˜ ë¬¼ì§ˆì´ë‚˜ ì—ë„ˆì§€ê°€ ì‘ìš©í•˜ì—¬ ì¤‘ì‹¬ì— ë§¤ìš° ë°€ë„ ë†’ì€ ì§ˆëŸ‰ì´ ëª¨ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì…ë‹ˆë‹¤. ë¸”ë™í™€ì˜ ì¤‘ë ¥ì¥ì€ ì£¼ë³€ì— ìˆëŠ” ë¬¼ì²´ë‚˜ ë¹›ì¡°ì°¨ë„ í¡ìˆ˜í•˜ì—¬ íƒˆì¶œí•  ìˆ˜ ì—†ê²Œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ ë¸”ë™í™€ì€ ë¹›ë§ˆì €ë„ ê°‡ì•„ë²„ë¦°ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ë¸”ë™í™€ì€ ìš°ì£¼ì—ì„œ ê°€ì¥ ì´ìƒí•œ ì²œì²´ ì¤‘ í•˜ë‚˜ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "Using MATH\n",
      "ê²½ë¡œ í†µí•©ì´ë€ ê·¸ë˜í”„ ì´ë¡ ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°œë…ìœ¼ë¡œ, ê·¸ë˜í”„ì—ì„œ ë‘ ì •ì  ì‚¬ì´ì˜ ê²½ë¡œë¥¼ ì—°ê²°í•˜ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ì¦‰, í•˜ë‚˜ì˜ ê²½ë¡œê°€ ì—¬ëŸ¬ ê²½ë¡œë¥¼ í†µí•©í•˜ì—¬ ìµœë‹¨ ê²½ë¡œ ë˜ëŠ” íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ë¡œë¥¼ ì°¾ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê²½ë¡œ í†µí•©ì€ ë„¤íŠ¸ì›Œí¬ ìµœì í™”, ë°ì´í„° í†µì‹ , ë¼ìš°íŒ… ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‘ìš©ë˜ë©°, ê·¸ë˜í”„ ì´ë¡ ì˜ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ë¬¼ë¦¬í•™ ì§ˆë¬¸ì— ëŒ€í•œ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "physics_template = \"\"\"ë‹¹ì‹ ì€ ë§¤ìš° ë˜‘ë˜‘í•œ ë¬¼ë¦¬í•™ êµìˆ˜ì…ë‹ˆë‹¤. \n",
    "ë¬¼ë¦¬í•™ì— ëŒ€í•œ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ëŒ€ë‹µí•˜ëŠ” ë° ëŠ¥ìˆ™í•©ë‹ˆë‹¤. \n",
    "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ëª¨ë¥¼ ë•ŒëŠ” ëª¨ë¥¸ë‹¤ê³  ì¸ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì€ ì§ˆë¬¸ì…ë‹ˆë‹¤:\n",
    "{query}\"\"\"\n",
    "\n",
    "# ìˆ˜í•™ ì§ˆë¬¸ì— ëŒ€í•œ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "math_template = \"\"\"ë‹¹ì‹ ì€ ì•„ì£¼ í›Œë¥­í•œ ìˆ˜í•™ìì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ìˆ˜í•™ ë¬¸ì œì— ëŒ€í•œ ë‹µì„ ì˜í•©ë‹ˆë‹¤. \n",
    "ë‹¹ì‹ ì€ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ êµ¬ì„± ìš”ì†Œë¡œ ë¶„í•´í•˜ê³ , \"êµ¬ì„± ìš”ì†Œì— ë‹µí•œ ë‹¤ìŒ \"ë” ë„“ì€ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë§¤ìš° í›Œë¥­í•©ë‹ˆë‹¤.\n",
    "êµ¬ì„± ìš”ì†Œì— ë‹µí•œ ë‹¤ìŒ ì´ë¥¼ ì¢…í•©í•˜ì—¬ ë” ë„“ì€ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸° ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "{query}\"\"\"\n",
    "\n",
    "# OpenAIEmbeddings ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# ë¬¼ë¦¬í•™ê³¼ ìˆ˜í•™ í…œí”Œë¦¿ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ìŠµë‹ˆë‹¤.\n",
    "prompt_templates = [physics_template, math_template]\n",
    "\n",
    "# í…œí”Œë¦¿ë“¤ì„ ì„ë² ë”©í•©ë‹ˆë‹¤.\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "# ì…ë ¥ëœ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ í…œí”Œë¦¿ì„ ì„ íƒí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "def prompt_router(input):\n",
    "    # ì…ë ¥ëœ ì§ˆë¬¸ì„ ì„ë² ë”©í•©ë‹ˆë‹¤.\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    \n",
    "    # ì§ˆë¬¸ ì„ë² ë”©ê³¼ í…œí”Œë¦¿ ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    \n",
    "    # ê°€ì¥ ìœ ì‚¬í•œ í…œí”Œë¦¿ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    \n",
    "    # ì„ íƒëœ í…œí”Œë¦¿ì´ ìˆ˜í•™ í…œí”Œë¦¿ì¸ì§€ ë¬¼ë¦¬í•™ í…œí”Œë¦¿ì¸ì§€ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "\n",
    "    # ì„ íƒëœ í…œí”Œë¦¿ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "# ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "chain = (\n",
    "    { \"query\": RunnablePassthrough() }  # ì…ë ¥ëœ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤.\n",
    "    | RunnableLambda(prompt_router)  # ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ í…œí”Œë¦¿ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    | ChatOpenAI()  # ì„ íƒëœ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ OpenAIì™€ ëŒ€í™”í•©ë‹ˆë‹¤.\n",
    "    | StrOutputParser()  # ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    ")\n",
    "\n",
    "# ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(chain.invoke(\"ë¸”ë™í™€ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\"))\n",
    "print(chain.invoke(\"ê²½ë¡œ í†µí•©ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a8df6-1749-449f-ba88-6176dd270aee",
   "metadata": {},
   "source": [
    "### 10. RAG-ëŒ€í•™êµ í•™ìƒ ì„±ì ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë¥¼ íƒìƒ‰í•˜ê³  ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5239853-f9f9-4601-a5ea-ae28cd7d15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€í•™ìƒì˜ í•™ì—…ì„±ì·¨ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì‹¬ë¦¬ì , í•™ìŠµì—­ëŸ‰, í™˜ê²½/ì§€ì§€ ìš”ì¸ íƒìƒ‰  289\n",
      "2. í•™ì—…ì„±ì·¨ì™€ í•™ìŠµì—­ëŸ‰ ë³€ì¸ê³¼ì˜ ê´€ê³„\n",
      "ëŒ€í•™ìƒì˜ í•™ì—…ì„±ì·¨ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì£¼ëŠ” í•™ìŠµì—­ëŸ‰ ë³€ì¸ìœ¼ë¡œëŠ” í•™ìŠµë™ê¸°ì™€ í•™ìŠµìŠµê´€ì´ ìˆ\n",
      "ë‹¤. ë™ê¸°ëŠ” í•™ìŠµê³¼ì •ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆëŠ”ë°, í•™ìŠµì— ìˆì–´ì„œ ë™ê¸°ëŠ” ì–´ë–¤ ë‚´ìš©ì„ ì„ íƒí•˜ê³ , ì–¸\n",
      "ì œ ê³µë¶€ë¥¼ í•˜ë©°, ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ê³µë¶€í•˜ëŠ”ì§€ ë“± ê·¸ ê³¼ì • ì „ë°˜ì— ì˜í–¥ì„ ì¤€ë‹¤(Schunk, 1989). í•™ìŠµ\n",
      "ì€ ì˜ì‹ì ì´ê³  ì˜ë„ì ì¸ í™œë™ì´ ìš”êµ¬ë˜ëŠ” ëŠ¥ë™ì ì¸ ê³¼ì •ì´ë¯€ë¡œ ê°œì¸ì˜ í™œë™ì„ ìœ ë°œì‹œí‚¤ê³  ìœ ì§€\n",
      "í•˜ë©°, ì´ëŸ¬í•œ í™œë™ì„ í†µí•´ ì–´ë–¤ ëª©í‘œ\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "#\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader  # PDF ë¡œë” ì¶”ê°€\n",
    "from langchain.chains import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ: ì™¸ë¶€ ë°ì´í„°ë¡œ í•™ìŠµ ìë£Œ ë¡œë“œ\n",
    "loader = DirectoryLoader(\n",
    "    './student_grades_research_papers',\n",
    "    glob=\"**/*.pdf\",  # PDF íŒŒì¼ë§Œ ë¡œë“œ\n",
    "    loader_cls=PyMuPDFLoader  # PDF ë¡œë” ì‚¬ìš©\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# ë¬¸ì„œ ë‚´ìš© í™•ì¸ (ì˜ˆ: 5ë²ˆì§¸ ë¬¸ì„œì˜ ì²« 300ì ì¶œë ¥)\n",
    "if len(documents) > 4:\n",
    "    print(documents[4].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7ef1282-2f25-4d51-b1ce-dcf030f86453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ë¬¸ì„œ ì„ë² ë”©: ë¬¸ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 3. ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# 4. LLM ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"documents\"],\n",
    "    template=\"ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€í•™êµ í•™ìƒì˜ ì„±ì ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë“¤ì„ ì„¤ëª…í•´ ì¤˜: {documents}\"\n",
    ")\n",
    "\n",
    "# LLMChain ìƒì„±\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# StuffDocumentsChain ìƒì„±\n",
    "combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain)\n",
    "\n",
    "# 5. ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ì—°ê²°í•˜ëŠ” RetrievalQA êµ¬ì„±\n",
    "qa_chain = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1922857d-fda0-466d-ba32-6443626c8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '\\nëŒ€í•™êµ í•™ìƒì˜ ì„±ì ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë“¤ì„ ì„¤ëª…í•´ ì¤˜. \\nê²€ìƒ‰ëœ ì—°êµ¬ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•´ ì£¼ê³ , ì„±ì  í–¥ìƒì— ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì „ëµì„ ì œì‹œí•´ ì¤˜.\\n', 'result': 'ëŒ€í•™ìƒì˜ ì„±ì ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë“¤ì€ ì—¬ëŸ¬ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìœ¼ë©°, ì£¼ë¡œ ì‹¬ë¦¬ì  ìš”ì¸, í•™ìŠµì—­ëŸ‰, í™˜ê²½ ë° ì§€ì§€ ìš”ì¸ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ê° ìš”ì†Œì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\\n\\n### 1. ì‹¬ë¦¬ì  ìš”ì¸\\n- **í•™ì—…ì  ìê¸°íš¨ëŠ¥ê°**: í•™ìƒì´ ìì‹ ì˜ í•™ì—… ëŠ¥ë ¥ì— ëŒ€í•´ ê°€ì§€ëŠ” ì‹ ë…ìœ¼ë¡œ, ë†’ì€ ìê¸°íš¨ëŠ¥ê°ì€ í•™ì—… ì„±ì·¨ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ìì‹ ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ë¯¿ìŒì€ ë„ì „ì ì¸ ìƒí™©ì—ì„œë„ ì§€ì†ì ìœ¼ë¡œ ë…¸ë ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\\n- **ì„±ì‹¤ì„±**: í•™ì—…ì— ëŒ€í•œ ê¾¸ì¤€í•œ ë…¸ë ¥ê³¼ ì±…ì„ê°ì€ ì„±ì  í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì„±ì‹¤í•œ í•™ìƒì€ í•™ìŠµì— ë” ë§ì€ ì‹œê°„ì„ íˆ¬ìí•˜ê³ , ê³¼ì œë¥¼ ì„±ì‹¤íˆ ìˆ˜í–‰í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.\\n- **í•™ìŠµë™ê¸°**: í•™ìŠµì— ëŒ€í•œ ë‚´ì  ë™ê¸°ì™€ ì™¸ì  ë™ê¸°ëŠ” í•™ìƒì˜ í•™ì—… ì„±ì·¨ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë†’ì€ í•™ìŠµë™ê¸°ë¥¼ ê°€ì§„ í•™ìƒì€ ë” ë§ì€ ë…¸ë ¥ì„ ê¸°ìš¸ì´ê³ , í•™ìŠµì— ëŒ€í•œ í¥ë¯¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\\n\\n### 2. í•™ìŠµì—­ëŸ‰\\n- **í•™ìŠµìŠµê´€**: íš¨ê³¼ì ì¸ í•™ìŠµìŠµê´€ì€ ì„±ì  í–¥ìƒì— ê¸°ì—¬í•©ë‹ˆë‹¤. ì •ê¸°ì ì¸ ë³µìŠµ, ì‹œê°„ ê´€ë¦¬, ëª©í‘œ ì„¤ì • ë“±ì€ í•™ìŠµì˜ ì§ˆì„ ë†’ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\\n- **ìê¸°ì¡°ì ˆí•™ìŠµ**: ìì‹ ì˜ í•™ìŠµ ê³¼ì •ì„ ê³„íší•˜ê³  ì¡°ì ˆí•˜ëŠ” ëŠ¥ë ¥ì€ ì„±ì ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ìê¸°ì¡°ì ˆí•™ìŠµì´ ì˜ ì´ë£¨ì–´ì§€ëŠ” í•™ìƒì€ í•™ìŠµ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³ , ê·¸ì— ë§ì¶° í•™ìŠµ ì „ëµì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n### 3. í™˜ê²½ ë° ì§€ì§€ ìš”ì¸\\n- **êµìˆ˜ì§€ì§€**: êµìˆ˜ì™€ì˜ ê¸ì •ì ì¸ ê´€ê³„ëŠ” í•™ìƒì˜ í•™ì—… ì„±ì·¨ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. êµìˆ˜ì˜ í”¼ë“œë°±ê³¼ ì§€ì›ì€ í•™ìƒì´ í•™ìŠµì— ëŒ€í•œ ìì‹ ê°ì„ ê°–ê²Œ í•˜ê³ , ë¬¸ì œ í•´ê²°ì— ë„ì›€ì„ ì¤ë‹ˆë‹¤.\\n- **í•™ìŠµí™˜ê²½**: ë¬¼ë¦¬ì  í™˜ê²½(ì˜ˆ: ë„ì„œê´€, ê°•ì˜ì‹¤)ê³¼ ì‚¬íšŒì  í™˜ê²½(ì˜ˆ: ì¹œêµ¬, ê°€ì¡±)ì˜ ì§ˆì€ í•™ìŠµì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ì§€ì›ì ì¸ í•™ìŠµ í™˜ê²½ì€ í•™ìƒì´ ë” ë‚˜ì€ ì„±ì ì„ ì–»ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\\n- **ì‚¬íšŒì  ì§€ì§€**: ì¹œêµ¬, ê°€ì¡±, ë™ë£Œë¡œë¶€í„°ì˜ ì •ì„œì  ë° ì‹¤ì§ˆì ì¸ ì§€ì›ì€ í•™ìƒì˜ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ê³ , í•™ì—…ì— ëŒ€í•œ ê¸ì •ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.\\n\\n### ê²°ë¡ \\nëŒ€í•™ìƒì˜ ì„±ì ì€ ì‹¬ë¦¬ì  ìš”ì¸, í•™ìŠµì—­ëŸ‰, í™˜ê²½ ë° ì§€ì§€ ìš”ì¸ ë“± ë‹¤ì–‘í•œ ìš”ì†Œì˜ ìƒí˜¸ì‘ìš©ì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, í•™ìƒ ê°œê°œì¸ì˜ íŠ¹ì„±ê³¼ ìƒí™©ì— ë§ì¶˜ ë§ì¶¤í˜• ì§€ì›ì´ í•„ìš”í•˜ë©°, ì´ëŸ¬í•œ ìš”ì†Œë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ì—°êµ¬ì™€ í”„ë¡œê·¸ë¨ ê°œë°œì´ ì¤‘ìš”í•©ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# 6. Prompt ì„¤ì •\n",
    "prompt = \"\"\"\n",
    "ëŒ€í•™êµ í•™ìƒì˜ ì„±ì ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë“¤ì„ ì„¤ëª…í•´ ì¤˜. \n",
    "ê²€ìƒ‰ëœ ì—°êµ¬ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•´ ì£¼ê³ , ì„±ì  í–¥ìƒì— ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì „ëµì„ ì œì‹œí•´ ì¤˜.\n",
    "\"\"\"\n",
    "\n",
    "# 7. ì§ˆë¬¸ ì‹¤í–‰\n",
    "try:\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "    retrieved_documents = retriever.get_relevant_documents(prompt)  # promptë¡œ ê²€ìƒ‰ ì‹¤í–‰\n",
    "    \n",
    "    # ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    if not retrieved_documents:\n",
    "        raise ValueError(\"ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    documents_text = \"\\n\".join(\n",
    "        [doc.page_content for doc in retrieved_documents if isinstance(doc.page_content, str) and doc.page_content]\n",
    "    )  # ë¹„ì–´ìˆì§€ ì•Šì€ ë¬¸ì„œ ë‚´ìš©ë§Œ í¬í•¨\n",
    "    \n",
    "    if not documents_text.strip():  # ë¬¸ì„œ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸\n",
    "        raise ValueError(\"ë¬¸ì„œ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì§ˆë¬¸ ì‹¤í–‰\n",
    "    result = qa_chain.invoke({\"query\": prompt})  # promptë¡œ query ì „ë‹¬\n",
    "    print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ì§ˆë¬¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631dda7-b30d-40c6-8ce2-edbc192ada4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_langchain",
   "language": "python",
   "name": "py310_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
